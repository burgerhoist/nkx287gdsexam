{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports isolated here so on restart only this has to be run\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1...\n",
      "Processed chunk 1.\n",
      "Processing chunk 2...\n",
      "Processed chunk 2.\n",
      "Processing chunk 3...\n",
      "Processed chunk 3.\n",
      "Processing chunk 4...\n",
      "Processed chunk 4.\n",
      "Processing chunk 5...\n",
      "Processed chunk 5.\n",
      "Processing chunk 6...\n",
      "Processed chunk 6.\n",
      "Processing chunk 7...\n",
      "Processed chunk 7.\n",
      "Processing chunk 8...\n",
      "Processed chunk 8.\n",
      "Processing chunk 9...\n",
      "Processed chunk 9.\n",
      "Processing chunk 10...\n",
      "Processed chunk 10.\n",
      "Processing chunk 11...\n",
      "Processed chunk 11.\n",
      "Processing chunk 12...\n",
      "Processed chunk 12.\n",
      "Processing chunk 13...\n",
      "Processed chunk 13.\n",
      "Processing chunk 14...\n",
      "Processed chunk 14.\n",
      "Processing chunk 15...\n",
      "Processed chunk 15.\n",
      "Processing chunk 16...\n",
      "Processed chunk 16.\n",
      "Processing chunk 17...\n",
      "Processed chunk 17.\n",
      "Processing chunk 18...\n",
      "Processed chunk 18.\n",
      "Processing chunk 19...\n",
      "Processed chunk 19.\n",
      "Processing chunk 20...\n",
      "Processed chunk 20.\n",
      "Processing chunk 21...\n",
      "Processed chunk 21.\n",
      "Processing chunk 22...\n",
      "Processed chunk 22.\n",
      "Processing chunk 23...\n",
      "Processed chunk 23.\n",
      "Processing chunk 24...\n",
      "Processed chunk 24.\n",
      "Processing chunk 25...\n",
      "Processed chunk 25.\n",
      "Processing chunk 26...\n",
      "Processed chunk 26.\n",
      "Processing chunk 27...\n",
      "Processed chunk 27.\n",
      "Processing chunk 28...\n",
      "Processed chunk 28.\n",
      "Processing chunk 29...\n",
      "Processed chunk 29.\n",
      "Processing chunk 30...\n",
      "Processed chunk 30.\n",
      "Processing chunk 31...\n",
      "Processed chunk 31.\n",
      "Processing chunk 32...\n",
      "Processed chunk 32.\n",
      "Processing chunk 33...\n",
      "Processed chunk 33.\n",
      "Processing chunk 34...\n",
      "Processed chunk 34.\n",
      "Processing chunk 35...\n",
      "Processed chunk 35.\n",
      "Processing chunk 36...\n",
      "Processed chunk 36.\n",
      "Processing chunk 37...\n",
      "Processed chunk 37.\n",
      "Processing chunk 38...\n",
      "Processed chunk 38.\n",
      "Processing chunk 39...\n",
      "Processed chunk 39.\n",
      "Processing chunk 40...\n",
      "Processed chunk 40.\n",
      "Processing chunk 41...\n",
      "Processed chunk 41.\n",
      "Processing chunk 42...\n",
      "Processed chunk 42.\n",
      "Processing chunk 43...\n",
      "Processed chunk 43.\n",
      "Processing chunk 44...\n",
      "Processed chunk 44.\n",
      "Processing chunk 45...\n",
      "Processed chunk 45.\n",
      "Processing chunk 46...\n",
      "Processed chunk 46.\n",
      "Processing chunk 47...\n",
      "Processed chunk 47.\n",
      "Processing chunk 48...\n",
      "Processed chunk 48.\n",
      "Processing chunk 49...\n",
      "Processed chunk 49.\n",
      "Processing chunk 50...\n",
      "Processed chunk 50.\n",
      "Processing chunk 51...\n",
      "Processed chunk 51.\n",
      "Processing chunk 52...\n",
      "Processed chunk 52.\n",
      "Processing chunk 53...\n",
      "Processed chunk 53.\n",
      "Processing chunk 54...\n",
      "Processed chunk 54.\n",
      "Processing chunk 55...\n",
      "Processed chunk 55.\n",
      "Processing chunk 56...\n",
      "Processed chunk 56.\n",
      "Processing chunk 57...\n",
      "Processed chunk 57.\n",
      "Processing chunk 58...\n",
      "Processed chunk 58.\n",
      "Processing chunk 59...\n",
      "Processed chunk 59.\n",
      "Processing chunk 60...\n",
      "Processed chunk 60.\n",
      "Processing chunk 61...\n",
      "Processed chunk 61.\n",
      "Processing chunk 62...\n",
      "Processed chunk 62.\n",
      "Processing chunk 63...\n",
      "Processed chunk 63.\n",
      "Processing chunk 64...\n",
      "Processed chunk 64.\n",
      "Processing chunk 65...\n",
      "Processed chunk 65.\n",
      "Processing chunk 66...\n",
      "Processed chunk 66.\n",
      "Processing chunk 67...\n",
      "Processed chunk 67.\n",
      "Processing chunk 68...\n",
      "Processed chunk 68.\n",
      "Processing chunk 69...\n",
      "Processed chunk 69.\n",
      "Processing chunk 70...\n",
      "Processed chunk 70.\n",
      "Processing chunk 71...\n",
      "Processed chunk 71.\n",
      "Processing chunk 72...\n",
      "Processed chunk 72.\n",
      "Processing chunk 73...\n",
      "Processed chunk 73.\n",
      "Processing chunk 74...\n",
      "Processed chunk 74.\n",
      "Processing chunk 75...\n",
      "Processed chunk 75.\n",
      "Processing chunk 76...\n",
      "Processed chunk 76.\n",
      "Processing chunk 77...\n",
      "Processed chunk 77.\n",
      "Processing chunk 78...\n",
      "Processed chunk 78.\n",
      "Processing chunk 79...\n",
      "Processed chunk 79.\n",
      "Processing chunk 80...\n",
      "Processed chunk 80.\n",
      "Processing chunk 81...\n",
      "Processed chunk 81.\n",
      "Processing chunk 82...\n",
      "Processed chunk 82.\n",
      "Processing chunk 83...\n",
      "Processed chunk 83.\n",
      "Processing chunk 84...\n",
      "Processed chunk 84.\n",
      "Processing chunk 85...\n",
      "Processed chunk 85.\n",
      "Processing chunk 86...\n",
      "Processed chunk 86.\n",
      "Processing chunk 87...\n",
      "Processed chunk 87.\n",
      "Processing chunk 88...\n",
      "Processed chunk 88.\n",
      "Processing chunk 89...\n",
      "Processed chunk 89.\n",
      "Processing chunk 90...\n",
      "Processed chunk 90.\n",
      "Processing chunk 91...\n",
      "Processed chunk 91.\n",
      "Processing chunk 92...\n",
      "Processed chunk 92.\n",
      "Processing chunk 93...\n",
      "Processed chunk 93.\n",
      "Processing chunk 94...\n",
      "Processed chunk 94.\n",
      "Processing chunk 95...\n",
      "Processed chunk 95.\n",
      "Processing chunk 96...\n",
      "Processed chunk 96.\n",
      "Processing chunk 97...\n",
      "Processed chunk 97.\n",
      "Processing chunk 98...\n",
      "Processed chunk 98.\n",
      "Processing chunk 99...\n",
      "Processed chunk 99.\n",
      "Processing chunk 100...\n",
      "Processed chunk 100.\n",
      "Training Accuracy: 0.83\n",
      "Validation Accuracy: 0.79\n",
      "Test Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data and putting it through the logistic regression\n",
    "\n",
    "# File path\n",
    "file_path = '995,000_rows.csv'\n",
    "\n",
    "def preprocess_content(content):\n",
    "    tokens = nltk.word_tokenize(content)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = [], [], [], [], [], []\n",
    "chunk_size = 10000\n",
    "current_chunk = 0\n",
    "\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    current_chunk += 1\n",
    "    print(f\"Processing chunk {current_chunk}...\")\n",
    "\n",
    "    chunk = chunk[['content', 'type']].dropna()  \n",
    "    chunk['content'] = chunk['content'].apply(preprocess_content)\n",
    "\n",
    "    X_chunk = chunk['content']\n",
    "    y_chunk = chunk['type']\n",
    "\n",
    "    X_train_chunk, X_temp, y_train_chunk, y_temp = train_test_split(X_chunk, y_chunk, test_size=0.2, random_state=42)\n",
    "    X_val_chunk, X_test_chunk, y_val_chunk, y_test_chunk = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    X_train.extend(X_train_chunk)\n",
    "    y_train.extend(y_train_chunk)\n",
    "    X_val.extend(X_val_chunk)\n",
    "    y_val.extend(y_val_chunk)\n",
    "    X_test.extend(X_test_chunk)\n",
    "    y_test.extend(y_test_chunk)\n",
    "\n",
    "    print(f\"Processed chunk {current_chunk}.\")\n",
    "\n",
    "X_train, X_val, X_test = pd.Series(X_train), pd.Series(X_val), pd.Series(X_test)\n",
    "y_train, y_val, y_test = pd.Series(y_train), pd.Series(y_val), pd.Series(y_test)\n",
    "\n",
    "\n",
    "train_data = pd.DataFrame({'content': X_train, 'type': y_train})\n",
    "val_data = pd.DataFrame({'content': X_val, 'type': y_val})\n",
    "test_data = pd.DataFrame({'content': X_test, 'type': y_test})\n",
    "\n",
    "train_data.to_csv('train_data.csv', index=False)\n",
    "val_data.to_csv('val_data.csv', index=False)\n",
    "test_data.to_csv('test_data.csv', index=False)\n",
    "\n",
    "\n",
    "model = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    LogisticRegression(max_iter=1000, random_state=42)\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving as a panda dataframe\n",
    "train_data = pd.DataFrame({'content': X_train, 'type': y_train})\n",
    "val_data = pd.DataFrame({'content': X_val, 'type': y_val})\n",
    "test_data = pd.DataFrame({'content': X_test, 'type': y_test})\n",
    "\n",
    "train_data.to_csv('train_data.csv', index=False)\n",
    "val_data.to_csv('val_data.csv', index=False)\n",
    "test_data.to_csv('test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens)\n\u001b[1;32m---> 12\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliar_train.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m val_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliar_valid.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)   \n\u001b[0;32m     14\u001b[0m test_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliar_test.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#Technically part of Part 4: Evaluation, but I included it in this notebook because it uses this model.\n",
    "\n",
    "\n",
    "def preprocess_content(content):\n",
    "    tokens = nltk.word_tokenize(content)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "train_data = pd.read_csv('liar_train.tsv', sep='\\t')\n",
    "val_data = pd.read_csv('liar_valid.tsv', sep='\\t')   \n",
    "test_data = pd.read_csv('liar_test.tsv', sep='\\t') \n",
    "\n",
    "train_data['content'] = train_data['content'].apply(preprocess_content)\n",
    "val_data['content'] = val_data['content'].apply(preprocess_content)\n",
    "test_data['content'] = test_data['content'].apply(preprocess_content)\n",
    "\n",
    "X_train = train_data['content']\n",
    "y_train = train_data['type']\n",
    "X_val = val_data['content']\n",
    "y_val = val_data['type']\n",
    "X_test = test_data['content']\n",
    "y_test = test_data['type']\n",
    "\n",
    "model = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    LogisticRegression(max_iter=1000, random_state=42)\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
