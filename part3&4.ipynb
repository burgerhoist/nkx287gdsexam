{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Datasets loaded.\n",
      "Filtering out invalid labels...\n",
      "Filtering complete.\n",
      "Encoding labels...\n",
      "Label encoding complete.\n",
      "Extracting features using TfidfVectorizer...\n",
      "Building SVM pipeline...\n",
      "SVM pipeline built.\n",
      "Training SVM model...\n",
      "Model training complete.\n",
      "Making predictions on validation data...\n",
      "Predictions on validation data complete.\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91     10621\n",
      "           1       0.95      0.96      0.96     21723\n",
      "\n",
      "    accuracy                           0.94     32344\n",
      "   macro avg       0.93      0.93      0.93     32344\n",
      "weighted avg       0.94      0.94      0.94     32344\n",
      "\n",
      "Performing hyperparameter tuning with GridSearchCV...\n",
      "Best parameters found: {'svc__C': 1}\n",
      "Making predictions on test data...\n",
      "Predictions on test data complete.\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91     10418\n",
      "           1       0.95      0.97      0.96     21934\n",
      "\n",
      "    accuracy                           0.94     32352\n",
      "   macro avg       0.94      0.93      0.93     32352\n",
      "weighted avg       0.94      0.94      0.94     32352\n",
      "\n",
      "Generating confusion matrix...\n",
      "Confusion Matrix:\n",
      "[[ 9311  1107]\n",
      " [  764 21170]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 2. Load your datasets (training, validation, and test data)\n",
    "print(\"Loading datasets...\")\n",
    "train_data = pd.read_csv('train_data.csv')  # Load your training data here\n",
    "val_data = pd.read_csv('val_data.csv')      # Load your validation data here\n",
    "test_data = pd.read_csv('test_data.csv')    # Load your test data here\n",
    "print(\"Datasets loaded.\")\n",
    "\n",
    "# 4. Define valid labels for the 'type' column (adjust as needed)\n",
    "valid_labels = ['fake', 'reliable']  # Define the valid labels you expect\n",
    "\n",
    "# 5. Filter out rows with invalid labels\n",
    "print(\"Filtering out invalid labels...\")\n",
    "train_data = train_data[train_data['type'].isin(valid_labels)]\n",
    "val_data = val_data[val_data['type'].isin(valid_labels)]\n",
    "test_data = test_data[test_data['type'].isin(valid_labels)]\n",
    "print(\"Filtering complete.\")\n",
    "\n",
    "# 7. Label encoding: Fit on training data and then transform on validation and test data\n",
    "print(\"Encoding labels...\")\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on training labels only\n",
    "train_data['type'] = label_encoder.fit_transform(train_data['type'])\n",
    "\n",
    "# Transform the validation and test labels\n",
    "val_data['type'] = label_encoder.transform(val_data['type'])\n",
    "test_data['type'] = label_encoder.transform(test_data['type'])\n",
    "print(\"Label encoding complete.\")\n",
    "\n",
    "# 8. Feature Extraction: Using TfidfVectorizer\n",
    "# Reduce the number of features to make it faster, also exclude stop words\n",
    "print(\"Extracting features using TfidfVectorizer...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)  # Reduce features to 1000\n",
    "\n",
    "# 9. Build the SVM model pipeline (vectorization + SVM classifier)\n",
    "print(\"Building SVM pipeline...\")\n",
    "svm_model = make_pipeline(tfidf_vectorizer, SVC(kernel='linear', random_state=42, C=1))  # Start with linear kernel and C=1 for speed\n",
    "print(\"SVM pipeline built.\")\n",
    "\n",
    "# 10. Train the SVM model on training data\n",
    "print(\"Training SVM model...\")\n",
    "svm_model.fit(train_data['content'], train_data['type'])\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# 11. Predict on validation data\n",
    "print(\"Making predictions on validation data...\")\n",
    "val_predictions = svm_model.predict(val_data['content'])\n",
    "print(\"Predictions on validation data complete.\")\n",
    "\n",
    "# 12. Evaluate on validation data\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(val_data['type'], val_predictions))\n",
    "\n",
    "# 13. Hyperparameter Tuning (Optional): Example with GridSearchCV\n",
    "print(\"Performing hyperparameter tuning with GridSearchCV...\")\n",
    "parameters = {'svc__C': [0.1, 1]}  # Reduce search space for speed (no 'rbf' kernel)\n",
    "grid_search = GridSearchCV(svm_model, parameters, cv=3, n_jobs=-1)  # Use multiple cores for grid search\n",
    "grid_search.fit(train_data['content'], train_data['type'])\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "\n",
    "# 14. Retrain model with best parameters from GridSearchCV (optional step)\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "\n",
    "# 15. Test the final model on the test data\n",
    "print(\"Making predictions on test data...\")\n",
    "test_predictions = best_svm_model.predict(test_data['content'])\n",
    "print(\"Predictions on test data complete.\")\n",
    "\n",
    "# 16. Evaluate on test data\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(test_data['type'], test_predictions))\n",
    "\n",
    "# 17. Confusion Matrix (optional)\n",
    "print(\"Generating confusion matrix...\")\n",
    "conf_matrix = confusion_matrix(test_data['type'], test_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model...\n",
      "Model saved.\n",
      "Saving label encoder...\n",
      "Label encoder saved.\n",
      "Confusion Matrix:\n",
      "          fake  reliable\n",
      "fake      9311      1107\n",
      "reliable   764     21170\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the best model after hyperparameter tuning\n",
    "print(\"Saving the model...\")\n",
    "joblib.dump(best_svm_model, 'best_svm_model.pkl')\n",
    "print(\"Model saved.\")\n",
    "\n",
    "# If you want to save the entire pipeline (including the vectorizer and classifier):\n",
    "joblib.dump(svm_model, 'svm_pipeline.pkl')\n",
    "\n",
    "# Save the label encoder\n",
    "print(\"Saving label encoder...\")\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "print(\"Label encoder saved.\")\n",
    "\n",
    "# Save classification report for validation and test\n",
    "with open('validation_classification_report.txt', 'w') as f:\n",
    "    f.write(\"Validation Classification Report:\\n\")\n",
    "    f.write(classification_report(val_data['type'], val_predictions))\n",
    "\n",
    "with open('test_classification_report.txt', 'w') as f:\n",
    "    f.write(\"Test Classification Report:\\n\")\n",
    "    f.write(classification_report(test_data['type'], test_predictions))\n",
    "\n",
    "# Save confusion matrix as CSV\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=label_encoder.classes_, columns=label_encoder.classes_)\n",
    "conf_matrix_df.to_csv('confusion_matrix.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reload the saved model\n",
    "best_svm_model = joblib.load('best_svm_model.pkl')\n",
    "# Or reload the full pipeline\n",
    "svm_model = joblib.load('svm_pipeline.pkl')\n",
    "\n",
    "# Reload the label encoder\n",
    "label_encoder = joblib.load('label_encoder.pkl')\n",
    "\n",
    "# Reload classification reports (if needed)\n",
    "with open('validation_classification_report.txt', 'r') as f:\n",
    "    validation_report = f.read()\n",
    "\n",
    "with open('test_classification_report.txt', 'r') as f:\n",
    "    test_report = f.read()\n",
    "\n",
    "# Reload confusion matrix\n",
    "conf_matrix_df = pd.read_csv('confusion_matrix.csv', index_col=0)\n",
    "\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Datasets loaded.\n",
      "Filtering out invalid labels...\n",
      "Filtering complete.\n",
      "Encoding labels...\n",
      "Label encoding complete.\n",
      "Extracting features using TfidfVectorizer...\n",
      "Building SVM pipeline...\n",
      "SVM pipeline built.\n",
      "Training SVM model...\n",
      "Model training complete.\n",
      "Making predictions on validation data...\n",
      "Predictions on validation data complete.\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.14      0.16       236\n",
      "           1       0.25      0.35      0.29       263\n",
      "           2       0.24      0.29      0.26       248\n",
      "           3       0.27      0.25      0.26       251\n",
      "           4       0.36      0.07      0.12       116\n",
      "           5       0.19      0.18      0.18       169\n",
      "\n",
      "    accuracy                           0.23      1283\n",
      "   macro avg       0.25      0.21      0.21      1283\n",
      "weighted avg       0.24      0.23      0.23      1283\n",
      "\n",
      "Performing hyperparameter tuning with GridSearchCV...\n",
      "Best parameters found: {'svc__C': 1}\n",
      "Making predictions on test data...\n",
      "Predictions on test data complete.\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.20      0.21       212\n",
      "           1       0.26      0.32      0.29       249\n",
      "           2       0.24      0.29      0.26       265\n",
      "           3       0.23      0.26      0.24       241\n",
      "           4       0.18      0.02      0.04        92\n",
      "           5       0.21      0.18      0.20       207\n",
      "\n",
      "    accuracy                           0.24      1266\n",
      "   macro avg       0.23      0.21      0.21      1266\n",
      "weighted avg       0.23      0.24      0.23      1266\n",
      "\n",
      "Generating confusion matrix...\n",
      "Confusion Matrix:\n",
      "[[42 51 62 39  2 16]\n",
      " [36 80 52 41  3 37]\n",
      " [36 52 76 64  2 35]\n",
      " [23 52 62 62  0 42]\n",
      " [26 23 20 11  2 10]\n",
      " [17 51 47 52  2 38]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 2. Load your datasets (training, validation, and test data)\n",
    "print(\"Loading datasets...\")\n",
    "train_data = pd.read_csv('liar_train.tsv', sep='\\t')  # Load your training data here\n",
    "val_data = pd.read_csv('liar_valid.tsv', sep='\\t')    # Load your validation data here\n",
    "test_data = pd.read_csv('liar_test.tsv', sep='\\t')    # Load your test data here\n",
    "print(\"Datasets loaded.\")\n",
    "\n",
    "train_data.columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job', 'state', 'party', 'score_1', 'score_2', 'score_3', 'score_4', 'score_5', 'source']\n",
    "val_data.columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job', 'state', 'party', 'score_1', 'score_2', 'score_3', 'score_4', 'score_5', 'source']\n",
    "test_data.columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job', 'state', 'party', 'score_1', 'score_2', 'score_3', 'score_4', 'score_5', 'source']\n",
    "\n",
    "\n",
    "# 3. Define valid labels for the 'label' column (adjust as needed)\n",
    "valid_labels = ['false', 'true', 'barely-true', 'mostly-true', 'half-true', 'pants-fire']  # Define the valid labels you expect\n",
    "\n",
    "# 4. Filter out rows with invalid labels\n",
    "print(\"Filtering out invalid labels...\")\n",
    "train_data = train_data[train_data['label'].isin(valid_labels)]\n",
    "val_data = val_data[val_data['label'].isin(valid_labels)]\n",
    "test_data = test_data[test_data['label'].isin(valid_labels)]\n",
    "print(\"Filtering complete.\")\n",
    "\n",
    "# 5. Label encoding: Fit on training data and then transform on validation and test data\n",
    "print(\"Encoding labels...\")\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on training labels only\n",
    "train_data['label'] = label_encoder.fit_transform(train_data['label'])\n",
    "\n",
    "# Transform the validation and test labels\n",
    "val_data['label'] = label_encoder.transform(val_data['label'])\n",
    "test_data['label'] = label_encoder.transform(test_data['label'])\n",
    "print(\"Label encoding complete.\")\n",
    "\n",
    "# 6. Feature Extraction: Using TfidfVectorizer\n",
    "# Reduce the number of features to make it faster, also exclude stop words\n",
    "print(\"Extracting features using TfidfVectorizer...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)  # Reduce features to 1000\n",
    "\n",
    "# 7. Build the SVM model pipeline (vectorization + SVM classifier)\n",
    "print(\"Building SVM pipeline...\")\n",
    "svm_model = make_pipeline(tfidf_vectorizer, SVC(kernel='linear', random_state=42, C=1))  # Start with linear kernel and C=1 for speed\n",
    "print(\"SVM pipeline built.\")\n",
    "\n",
    "# 8. Train the SVM model on training data\n",
    "print(\"Training SVM model...\")\n",
    "svm_model.fit(train_data['statement'], train_data['label'])  # Use 'statement' column for training\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# 9. Predict on validation data\n",
    "print(\"Making predictions on validation data...\")\n",
    "val_predictions = svm_model.predict(val_data['statement'])  # Use 'statement' column for validation\n",
    "print(\"Predictions on validation data complete.\")\n",
    "\n",
    "# 10. Evaluate on validation data\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(val_data['label'], val_predictions))\n",
    "\n",
    "# 11. Hyperparameter Tuning (Optional): Example with GridSearchCV\n",
    "print(\"Performing hyperparameter tuning with GridSearchCV...\")\n",
    "parameters = {'svc__C': [0.1, 1]}  # Reduce search space for speed (no 'rbf' kernel)\n",
    "grid_search = GridSearchCV(svm_model, parameters, cv=3, n_jobs=-1)  # Use multiple cores for grid search\n",
    "grid_search.fit(train_data['statement'], train_data['label'])\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "\n",
    "# 12. Retrain model with best parameters from GridSearchCV (optional step)\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "\n",
    "# 13. Test the final model on the test data\n",
    "print(\"Making predictions on test data...\")\n",
    "test_predictions = best_svm_model.predict(test_data['statement'])  # Use 'statement' column for testing\n",
    "print(\"Predictions on test data complete.\")\n",
    "\n",
    "# 14. Evaluate on test data\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(test_data['label'], test_predictions))\n",
    "\n",
    "# 15. Confusion Matrix (optional)\n",
    "print(\"Generating confusion matrix...\")\n",
    "conf_matrix = confusion_matrix(test_data['label'], test_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
